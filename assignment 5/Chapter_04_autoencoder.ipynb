{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Blackboard before the deadline (see Blackboard).\n",
    "    \n",
    "Learning goals:\n",
    "1. Derive and implement the mean-squared-error loss function and the rectified linear activation function\n",
    "1. Implement an autoencoder as a neural network with unsupervised learning\n",
    "1. Check what face features the autoencoder has learned to encode in the hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the architecture\n",
    "\n",
    "The autoencoder network that you are about to implement has the same structure as in the MLP exercise: The input, hidden, and output node layers are connected with two weight layers. This time, the $n_h$ hidden layer nodes use a *rectified linear* activation function, and the output units use *linear* activations. \n",
    "\n",
    "The data set is the Yale Face Database, which is a small set of grayscale photos of faces, each showing a different facial expression. The autoencoder will have as many input and output units as there are pixels. The hidden layer has far less units, as our goal is to learn a compressed representation of faces. The autoencoders job is \"learning to reconstruct\", that is, learning to reconstruct the face at the input units in the output units, after passing the information through the hidden units. Basically this means that `X` and `Y` are equal here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Mean squared error (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to predict pixel values we have a regression problem, and MSE is a convenient loss function for regression problems. MSE is large when the prediction is off, and near zero when the prediction is most similar to the target. The mean squared error (MSE) is given by:\n",
    "$$L = \\frac{1}{2N} \\sum^N (t^{(n)} - y^{(n)})^2$$\n",
    "Here, $t^{(n)}$ is the target vector, and $y^{(n)}$ the prediction of the $n$th example. \n",
    "\n",
    "We need the derivative of the MSE with respect to $y$ to do gradient descent (i.e., backpropagation) to tweak the parameters (i.e., the weights) for regression. Derive the MSE with respect to $y$, i.e. derive $\\frac{\\partial L}{\\partial y}$. You can ignore the batch index $n$ ($N = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1:\n",
    "$$\\frac{\\partial L}{\\partial y} =-(t^{(n)} - y^{(n)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: The rectified linear unit activation function (ReLU) (1 point)\n",
    "The rectified linear activation function is given by:\n",
    "$$f(a) = max(0, a)$$\n",
    "\n",
    "Obtain its derivative $\\frac{\\partial f}{\\partial a}$. \n",
    "\n",
    "Hint: It is a simple conditional expression (two cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2:\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial f}{\\partial a} &=& \n",
    "\\begin{cases}\n",
    "    1, & \\text{if } x > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: The forward pass (1 point)\n",
    "The network is defined as having two layers $w_1$ and $w_2$, where $w_1$ connects the input $x$ with the hidden layer $h$, and $w_2$ connects the hidden layer $h$ with the output layer $y$. For the hidden layer, we use rectified linear units with the activation function $f(a) = max(0, a)$. The output layer is just a linear layer (with the linear activation function $g(a) = a$). Write down the expressions for $a_1$, $h$, $a_2$, and $y$, where $a_1$ and $a_2$ are the activities of the hidden and output layer before passing them through the rectified linear and linear activations, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3:\n",
    "Write $\\LaTeX$ here.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "a_1 &=&  w1 * x \\\\\n",
    "h &=& f(a_1) = f(a) = max(0, a_1) \\\\\n",
    "a_2 &=&  w2 * h\\\\\n",
    "y &=& g(a_2) = a_2\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Gradient of the last layer (1 point)\n",
    "To compute the partial derivatives on the weights $w_2$ of the last layer, we have to propagate from the error function back through the non-linearity to the weights. Derive the chain of partial derivatives to compute $\\frac{\\partial L}{\\partial w_2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4\n",
    "$\\frac{\\partial L}{\\partial w_2}\\ = \\frac{\\partial y}{\\partial a_2}\\ * \\frac{\\partial a_2}{\\partial h}*  \\frac{\\partial L}{\\partial y}\\ = w_2 * -(t^{(n)} - y^{(n)})\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Gradient of the first layer (1 point)\n",
    "To compute the partial derivatives on the weights $w_1$ of the first layer, we have to propagate the error even further down the network. Derive the chain of partial derivatives to compute $\\frac{\\partial L}{\\partial w_1}$.\n",
    "\n",
    "Due to the 4 intermediate variables $y$, $a_2$, $h$ and $a_1$ the chain rule product will have 5 terms. Due do the derivative of the rectified linear activation, which is a conditional expression; you will end up with a conditional expression here too:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial L}{\\partial w_1} &=& \n",
    "\\begin{cases}\n",
    "    ?, & \\text{if } a_1 > 0 \\\\\n",
    "    ?, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_2}\\ = \\frac{\\partial y}{\\partial a_2}\\ * \\frac{\\partial a_2}{\\partial h}\\  * \\frac{\\partial h}{\\partial a_1} * \\frac{\\partial a_1}{\\partial w_1} * \\frac{\\partial L}{\\partial y} *  \\frac{\\partial y}{\\partial w_2}  \\ = \n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial L}{\\partial w_1} &=& \n",
    "\\begin{cases}\n",
    "    x*w_2*-(t^{(n)} - y^{(n)}), & \\text{if } a_1 > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Implementation (2.5 points)\n",
    "Now we can start implementing the autoencoder. Write the following functions:\n",
    "1. `mean_squared_error(Y, X)`: Computes the mean squared error. You need to sum over the pixel axis 0, and then mean  the result over the examples.\n",
    "1. `relu(A)`: Passes the activity `A` through the rectified linear unit. Use [`np.maximum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html) to compare each element of A to 0.\n",
    "1. `linear(X, W)`: Computes the activities `A` as `X` weighted by `W`. Just like in the previous exercises. \n",
    "1. `forward(X, W1, W2)`: Computes the forward pass for the two-layer AE with the `relu(A)` function at the hidden units. As the linear activation is just the trivial identity there is no activation function at the output units. Returns `A1`, `H` and `Y`. \n",
    "1. `backward(X, A1, H, Y, W2)`: Computes the backward pass for the two-layer AE with ReLU hidden units and MSE. The conditional can be realized by multiplying with the boolean matrix `(A1 > 0)`. \n",
    "1. `train_network(X_train, X_val,n_hidden, n_epochs, eta)`: Implement the training procedure (learn the weights). See the skeleton code for some help. Note that we have supplied a `initialize_weights(n_in, n_out)` function to initialize weights in the range from the MLP assignment. Remember that the input is equal to the output, so we do not need anything but `X_train` for training. \n",
    "\n",
    "Feel free to copy some code from the previous exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(n_in, n_out):\n",
    "    \"\"\"\n",
    "    Initializes a weight matrix.\n",
    "    INPUT:\n",
    "        n_in  = [int] number of input units.\n",
    "        n_out = [int] number of output units\n",
    "    OUTPUTS\n",
    "        W = [n_out n_in] the initial weight matrix\n",
    "    \"\"\"\n",
    "    r = np.sqrt(6) / np.sqrt(n_out + n_in)\n",
    "    return np.random.uniform(-r, r, [n_out, n_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(Y, X):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error.\n",
    "    INPUT:\n",
    "        Y = [P N] output vector for N examples of length P\n",
    "        X = [P N] target vector for N examples of length P\n",
    "    OUTPUTS\n",
    "        L = [flt] the MSE\n",
    "    \"\"\"\n",
    "    ## Write code here ##\n",
    "\n",
    "    L = -(X[1]-Y[1])\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(A):\n",
    "    \"\"\"\n",
    "    Computes the rectified linear activation.\n",
    "    INPUT:\n",
    "        A = [K N] activity matrix of K units for N examples\n",
    "    OUTPUT\n",
    "        Y = [K N] output matrix of K units for N examples\n",
    "    \"\"\"\n",
    "    ## Write code here ##\n",
    "    if np.max(A)>0:\n",
    "        Y=1\n",
    "    else:\n",
    "        Y=0\n",
    "    return Y\n",
    "    \n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(X, W):\n",
    "    \"\"\"\n",
    "    Computes the activities for a fully connected layer.\n",
    "    INPUT:\n",
    "        X = [P N] data matrix of P input units for N examples\n",
    "        W = [Q P] weight matrix of P inputs to Q outputs\n",
    "    OUTPUT\n",
    "        A = [Q N] activity matrix of Q output units for N examples\n",
    "    \"\"\"\n",
    "    ## Write code here ##\n",
    "    A = np.dot(W, X)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(X, W1, W2):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a two-layer AE with relu hidden units.\n",
    "    INPUT\n",
    "        X  = [P N] data matrix of P inputs for N examples\n",
    "        W1 = [Q P] weight matrix of the first layer from P input pixels to Q outputs\n",
    "        W2 = [P Q] weight vector of the second layer of Q inputs to P output pixels\n",
    "    OUTPUT\n",
    "        A1  = [Q N] linear activations going into the hidden unit activation functions for P pixels and N examples\n",
    "        H  = [Q N] output matrix of Q hidden units for N examples\n",
    "        Y  = [P N] output pixel vectors (reconstructions) for N examples\n",
    "    \"\"\"    \n",
    "\n",
    "    ## Write code here ##\n",
    "    A1 = linear(X, W1)\n",
    "    H = relu(A1)\n",
    "    A2 = linear(H, W2)\n",
    "    Y = mean_squared_error(A2, X)\n",
    "\n",
    "    return A1, H, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, A1, H, Y, W2):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a two-layer network with sigmoid and softmax units, and cross-entropy loss.  \n",
    "    INPUT:\n",
    "        X  = [P N] data matrix of P inputs for N examples\n",
    "        H  = [Q N] output matrix of Q hidden units for N examples\n",
    "        Y  = [P N] output pixel vectors of length P for N examples\n",
    "        W2 = [P Q] weight vector of the second layer of Q inputs to P outputs\n",
    "    OUTPUT\n",
    "        dW1 = [Q P] gradient matrix for the weights of layer 1 of P inputs to Q outputs\n",
    "        dW2 = [P Q] gradient matrix for the weights of layer 2 of Q inputs to P outputs\n",
    "        A1  = [Q N] activity matrix of Q hidden units for N examples\"\n",
    "    \"\"\"\n",
    "\n",
    "    ## Write code here ##\n",
    "    \n",
    "    if np.max (A1)<=0:\n",
    "        return 0\n",
    "\n",
    "    else:\n",
    "\n",
    "        dW2 = np.dot((Y-T),H.T)\n",
    "        dW1A = np.dot((Y-T).T,A1)\n",
    "#     dW1B = dW1A.T*H*(1-H)\n",
    "        dW1 = A1\n",
    " \n",
    "    return dW1, dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X_train, X_val, n_hidden=30, n_epochs=2000, eta=10**-5):\n",
    "    \"\"\"\n",
    "    Performs the training procedure for a two-layer MLP with ReLU hidden units and MSE.\n",
    "    INPUT:\n",
    "        X_train  = [P N] training data matrix of P inputs for N training examples\n",
    "        X_val    = [P N] validation data matrix of P inputs for N training examples\n",
    "        n_hidden = [int] number of hidden units (default 30)\n",
    "        n_epochs = [int] number of training epochs (default 2000)\n",
    "        eta      = [flt] learning rate (default 10^-5)\n",
    "    OUTPUT:\n",
    "        W1         = [Q P] the learned weights for layer 1 of P inputs to Q outputs\n",
    "        W2         = [P Q] the learned weights for layer 2 of Q inputs to P output pixels\n",
    "        train_loss = [Z 1] the training loss for Z epochs\n",
    "        val_loss   = [Z 1] the validation loss for Z epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize W1 and W2 (use initialize_weights())\n",
    "    ## Write code here ##\n",
    "    W1 = initialize_weights(X_train.shape[0], n_hidden)\n",
    "    W2 = initialize_weights(n_hidden, 10)\n",
    "    \n",
    "    # Loop over epochs\n",
    "    train_loss = np.zeros((n_epochs))\n",
    "    val_loss = np.zeros((n_epochs))\n",
    "    for i_epoch in xrange(n_epochs):\n",
    "        \n",
    "        # Forward pass\n",
    "        ## Write code here ##\n",
    "        H, Y, A1= forward(X_train, W1, W2)\n",
    "        Y_val = forward(X_val, W1, W2)\n",
    "        \n",
    "        # Backward pass\n",
    "        ## Write code here ##\n",
    "        dW1, dW2 = backward(X, A1, H, Y, W2)\n",
    "        \n",
    "        # Parameter update\n",
    "        ## Write code here ##\n",
    "        W1 = W1 - (eta*dW1)\n",
    "        W2 = W2 - (eta*dW2)\n",
    "        \n",
    "        # Save loss\n",
    "        ## Write code here ##\n",
    "        \n",
    "        # Print progress and loss\n",
    "        if i_epoch % 50 == 0:\n",
    "            print(\"Epoch {}/{}. Train loss: {}. Validation loss: {}.\".format(\n",
    "                1+i_epoch, n_epochs, train_loss[i_epoch], val_loss[i_epoch]))\n",
    "        \n",
    "    return W1, W2, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "In the next cells we load and prepare the face dataset. We also show example faces from the data set. Then we remove the mean from the data, and divide it by its standard deviation. This process, leading to a data set with zero-mean and unit variance is almost always an important preprocessing step. No information gets lost, but many common machine learning methods expect such *standardized* or *z-transformed* data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of face data set: (768L, 165L).\n"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "maxsz = [32, 32]\n",
    "X = []\n",
    "i = 0\n",
    "for file_name in os.listdir(os.path.join(os.getcwd(), \"yalefaces\")):\n",
    "    if file_name[:7] != \"subject\":\n",
    "        continue\n",
    "    im = Image.open(os.path.join(os.getcwd(), \"yalefaces\", file_name))\n",
    "    im.thumbnail(maxsz, Image.ANTIALIAS)\n",
    "    data = np.asarray(im)\n",
    "    if i == 0:\n",
    "        sz = data.shape\n",
    "    X.append(np.ndarray.flatten(data))\n",
    "    i += 1\n",
    "X = np.array(X).astype(\"float32\")\n",
    "X = X.T\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Dimensions of face data set: {}.\".format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAADiCAYAAABdqk9gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMJkFAAAAgMbCZBYAAAAAGsv/Aqms63o5YSzMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaca5048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot some examples\n",
    "fig, ax = plt.subplots(2, 5, figsize=(12, 4))\n",
    "ax = ax.flatten()\n",
    "for i_example in range(len(ax)):\n",
    "    ax[i_example].imshow(X[:, i_example*10].reshape((sz)), cmap=\"gray\")\n",
    "    ax[i_example].set_xticks([], [])\n",
    "    ax[i_example].set_yticks([], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardise data to zero mean and unit variance. We are doing this manually here as we need \n",
    "# mu and sigma to revert this operation (add mu, multiply by sigma) when \n",
    "# visualizing the resulting reconstructed faces. \n",
    "\n",
    "mu = X.mean(axis=1)\n",
    "sigma = X.std(axis=1)\n",
    "\n",
    "X -= np.expand_dims(mu, 1)\n",
    "X /= np.expand_dims(sigma, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions training inputs: (768L, 100L)\n",
      "Dimensions validation inputs: (768L, 40L)\n",
      "Dimensions testing inputs: (768L, 25L)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset in training, validation, and testing split\n",
    "X_train = X[:, :100]\n",
    "X_val = X[:, 100:140]\n",
    "X_test = X[:, 140:]\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Dimensions training inputs: {}\".format(X_train.shape))\n",
    "print(\"Dimensions validation inputs: {}\".format(X_val.shape))\n",
    "print(\"Dimensions testing inputs: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Running autoencoder training (0.5 points)\n",
    "\n",
    "Run the autoencoder with $n_h=30$ hidden units, a learning rate $\\eta=10^{-5}$ and for 2000 epochs. Plot the obtained training and validation losses again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,) (30,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-219-19eed28d8649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# print X.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# print Y.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Plot the training and validation losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-214-bba9bf38e4da>\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(X_train, X_val, n_hidden, n_epochs, eta)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m## Write code here ##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mY_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-212-29bb5747381c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(X, W1, W2)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mA2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mA1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-209-4658f64af513>\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(Y, X)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m## Write code here ##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,) (30,) "
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "## Write code here ##\n",
    "# print X.shape\n",
    "# print Y.shape\n",
    "W1, W2, train_loss, val_loss = train_network(X_train, X_val, n_hidden=30, n_epochs=2000, eta=10**-5)\n",
    "\n",
    "# Plot the training and validation losses\n",
    "## Write code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Reconstruction (1 point)\n",
    "\n",
    "Choose 5 faces from the set, and plot their original and their reconstructed image side-by-side. That is, plot a chosen face and the output your trained autoencoder (represented by the weights) creates for it.\n",
    "\n",
    "Use [`np.reshape`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html) to reshape pixel values for the input and output images to their original dimensions `sz` in a 2D array. Then make use of [`plt.imshow()`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.imshow.html) with a `gray` colormap to show the faces. \n",
    "\n",
    "You need to revert the scaling operation by multiplying by `sigma` and adding the `mean` (both previously computed) on each `x` and `y`. (If you don't, your real and reconstructed images will be off, e.g. too dark.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open a 5-by-2 figure with plt.subplots\n",
    "## Write code here ##\n",
    "\n",
    "# Loop over 5\n",
    "## Write code here ##\n",
    "\n",
    "    # Take a new example from X (the autoencoder input)\n",
    "    ## Write code here ##\n",
    "    \n",
    "    # Forward pass of X through the network, obtain the reconstruction y\n",
    "    ## Write code here ##\n",
    "    \n",
    "    # Plot the original X\n",
    "    ## Write code here ##\n",
    "    \n",
    "    # Plot the reconstructed X (output Y)\n",
    "    ## Write code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 9: Weights and receptive fields (1 point)\n",
    "We can visualize what was learned during classification and by the autoencoder. During training, the hidden units have become pattern detectors that you can interpret as *receptive fields*. \n",
    "\n",
    "You should visualize both `W1` and `W2`. To visualize the learned pattern of a single unit $i$, use [`np.reshape`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html) to reshape the weights leading from all image pixels to unit $i$ into a 2D array. Then make use of [`plt.imshow()`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.imshow.html) with a `gray` colormap to show the learned pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize W1\n",
    "## Write code here ##\n",
    "\n",
    "# Visualize W2\n",
    "## Write code here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
